{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#FFC300;\"></h1>\n",
    "<center><img src=\"./BCCC_banner_814.png\" alt=\"Header\" style=\"width: 814px;\"/></center>\n",
    "<h1 style=\"background-color:#FFC300;\"></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wispa, Wispa Gold or Crunchie ?!\n",
    "### Machine-Learning Computer-Vision Recognition Outreach Project for the Materials Made Smarter Centre adapted to support the Discover Materials Chocolate Impacter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an adaptation of the the Bottle, Can or Coffee Cup project described bellow with categories defined for recognising a Wispa bar, Wispa Gold bar or Crunchie Bar.\n",
    "\n",
    "This adaptation was created to demonstrate the alternative uses of the original outreach project live at the Festival of Tomorrow 21st and 22nd February 2025 whilst on the Discover Materials stand.\n",
    "\n",
    "The event is documented at https://discovermaterials.co.uk/news/discover-materials-ambassadors-at-the-festival-of-tomorrow/\n",
    "\n",
    "\n",
    "This project has been developed by the Materials Made Smarter Centre at Swansea University in collaboration with the Sustain Manufacturing Research Hub and Discover Materials to demonstrate how Computer Vision and Machine Learning can be used to recognise different objects to help with the sorting of materials for recycling.\n",
    "\n",
    "Further information and documentation about this project can be found at https://discovermaterials.co.uk/resource/bottle-can-or-coffee-cup/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"./Equipment_setup_814.jpg\" alt=\"Equipment\" style=\"width: 814px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Project Equipment, a portable monitor and two combined Seeed reComputer J1010 units hosting the NVIDIA Jetson Nano Jetpak project code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The platform this project is built on is the Seeed Studio reComputer J1010 NVIDIA Jetson Nano 2GB Platform with the Arm Cortex A57 CPU and NVIDIA Maxwell GPU and it has been developed by Dr R. Gibbs and Prof. C. Giannetti based upon the NVIDIA DLI \"Getting Started with AI on Jetson Nano‚Äù course which can be found in the ../classification directory\n",
    " \n",
    "Professor C. Giannetti would like to acknowledge the support of the EPSRC (EP/V061798/1) in this Materials Made Smarter Project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#FFC300;\"></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Camera\n",
    "This cell opens access to the Logitech C270 webcam attached to the USB3 port on reComputer 1. The jetcam library is contained within the nvidia_dli_docker environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Launch Camera ################################################################################\n",
    "\n",
    "# Check device number\n",
    "!ls -ltrh /dev/video*\n",
    "\n",
    "from jetcam.usb_camera import USBCamera\n",
    "\n",
    "# Logitech C270 webcam\n",
    "camera = USBCamera(width=224, height=224, capture_device=0) # confirm the capture_device number\n",
    "\n",
    "camera.running = True\n",
    "print(\"camera started\")\n",
    "\n",
    "################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Machine-Learning Task\n",
    "The task will be a Classification rather than a Regression Machine-Learning task. Classifying what the camera sees as either a bottle, a can or a coffee cup. There are three Datasets A, B or C. Dataset A contains 50 images each of the example props for the project. Datasets B and C can be used for other examples of training for new images. The images captured for training are stored in, for example, ./images/BCCC_A/Bottle/ etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Machine Learning Task ################################################################\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from dataset import ImageClassificationDataset\n",
    "\n",
    "TASK = 'WWGC'\n",
    "\n",
    "CATEGORIES = ['Wispa', 'Wispa_Gold', 'Crunchie']\n",
    "\n",
    "DATASETS = ['A', 'B', 'C']\n",
    "\n",
    "TRANSFORMS = transforms.Compose([\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "datasets = {}\n",
    "for name in DATASETS:\n",
    "    datasets[name] = ImageClassificationDataset('../data/images/' + TASK + '_' + name, CATEGORIES, TRANSFORMS)\n",
    "    \n",
    "print(\"{} task with {} categories defined\".format(TASK, CATEGORIES))\n",
    "\n",
    "# Set up the data directory location if not there already\n",
    "DATA_DIR = '/nvdli-nano/data/images/'\n",
    "!mkdir -p {DATA_DIR}\n",
    "\n",
    "################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Widgets used for Data Collection\n",
    "Creates the widgets that display the dataset and categories being collected, the image from the camera and the capture button which stores an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Data Collection Widget ################################################################\n",
    "\n",
    "import ipywidgets\n",
    "import traitlets\n",
    "from IPython.display import display\n",
    "from jetcam.utils import bgr8_to_jpeg\n",
    "\n",
    "# initialize active dataset\n",
    "dataset = datasets[DATASETS[0]]\n",
    "\n",
    "# unobserve all callbacks from camera in case we are running this cell for second time\n",
    "camera.unobserve_all()\n",
    "\n",
    "# create image from camera\n",
    "camera_widget = ipywidgets.Image()\n",
    "traitlets.dlink((camera, 'value'), (camera_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "# create widgets\n",
    "dataset_widget = ipywidgets.Dropdown(options=DATASETS, description='Dataset:')\n",
    "category_widget = ipywidgets.Dropdown(options=dataset.categories, description='Category:')\n",
    "count_widget = ipywidgets.IntText(description='# Images:')\n",
    "capture_widget = ipywidgets.Button(description='Capture Image')\n",
    "\n",
    "# update exisitng count of images at initialization\n",
    "count_widget.value = dataset.get_count(category_widget.value)\n",
    "\n",
    "# sets the active dataset\n",
    "def set_dataset(change):\n",
    "    global dataset\n",
    "    dataset = datasets[change['new']]\n",
    "    count_widget.value = dataset.get_count(category_widget.value)\n",
    "dataset_widget.observe(set_dataset, names='value')\n",
    "\n",
    "# update counts when we select a new category\n",
    "def update_counts(change):\n",
    "    count_widget.value = dataset.get_count(change['new'])\n",
    "category_widget.observe(update_counts, names='value')\n",
    "\n",
    "# save image for category and update counts\n",
    "def save(c):\n",
    "    dataset.save_entry(camera.value, category_widget.value)\n",
    "    count_widget.value = dataset.get_count(category_widget.value)\n",
    "capture_widget.on_click(save)\n",
    "\n",
    "data_collection_widget = ipywidgets.VBox([capture_widget,\n",
    "                                          dataset_widget,\n",
    "                                          category_widget,\n",
    "                                          count_widget])\n",
    "\n",
    "# output\n",
    "print(\"camera_widget, data_collection_widget created\")\n",
    "\n",
    "################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pre-trained RESNET 18 Neural Network Model\n",
    "There are several large Neural Network models which have been pre-trained on millions of general images to develop general understanding of what the camera is looking at. Through transfer learning, these pretrained models are used as the starting point from which new training is performed to specialise the recognition to a few specific categories. This permits successful models to be built using fewer training images than would be required if trying to learn a situation from scratch. It takes a short time to load in the model that is being used. Three other models are also available for experiementation. The models are stored in the nvidia_dli_docker environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "device = torch.device('cuda') # this makes use of the Graphical Processing Unit built into the Jetson Nano.\n",
    "\n",
    "# RESNET 18\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "model.fc = torch.nn.Linear(512, len(dataset.categories))\n",
    "\n",
    "# ALEXNET\n",
    "# model = torchvision.models.alexnet(pretrained=True)\n",
    "# model.classifier[-1] = torch.nn.Linear(4096, len(dataset.categories))\n",
    "\n",
    "# SQUEEZENET \n",
    "# model = torchvision.models.squeezenet1_1(pretrained=True)\n",
    "# model.classifier[1] = torch.nn.Conv2d(512, len(dataset.categories), kernel_size=1)\n",
    "# model.num_classes = len(dataset.categories)\n",
    "\n",
    "# RESNET 34\n",
    "# model = torchvision.models.resnet34(pretrained=True)\n",
    "# model.fc = torch.nn.Linear(512, len(dataset.categories))\n",
    "    \n",
    "model = model.to(device)\n",
    "\n",
    "model_save_button = ipywidgets.Button(description='save model')\n",
    "model_load_button = ipywidgets.Button(description='load model')\n",
    "model_path_widget = ipywidgets.Text(description='model path', value='/nvdli-nano/data/WWGC_model.pth')\n",
    "\n",
    "def load_model(c):\n",
    "    model.load_state_dict(torch.load(model_path_widget.value))\n",
    "model_load_button.on_click(load_model)\n",
    "    \n",
    "def save_model(c):\n",
    "    torch.save(model.state_dict(), model_path_widget.value)\n",
    "model_save_button.on_click(save_model)\n",
    "\n",
    "model_widget = ipywidgets.HBox([model_path_widget,model_load_button,model_save_button])\n",
    "\n",
    "# output\n",
    "print(\"model_widget created\")\n",
    "\n",
    "################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create State and Prediction Widgets\n",
    "Switch between Training and Prediction states and launch the live real-time state. In the real-time state the system displays a live view of what the camera sees, but waits until the 'recognise' button is pressed before performing a prediction/inference operation with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "\n",
    "import threading\n",
    "import time\n",
    "from utils import preprocess\n",
    "import torch.nn.functional as F\n",
    "\n",
    "state_widget = ipywidgets.ToggleButtons(options=['Train', 'Predict'], description='State', value='Train')\n",
    "prediction_widget = ipywidgets.Text(description='as') # this widget is placed after the 'Recognise' widget so that it reads Recognise as ...\n",
    "score_widgets = []\n",
    "for category in dataset.categories:\n",
    "    score_widget = ipywidgets.FloatSlider(min=0.0, max=1.0, description=category, orientation='vertical')\n",
    "    score_widgets.append(score_widget)\n",
    "    \n",
    "recognise_widget = ipywidgets.Button(description='Recognise')\n",
    "\n",
    "# run prediction on current image\n",
    "def recognise(c):\n",
    "    image = camera.value\n",
    "    preprocessed = preprocess(image)\n",
    "    output = model(preprocessed)\n",
    "    output = F.softmax(output, dim=1).detach().cpu().numpy().flatten()\n",
    "    category_index = output.argmax()\n",
    "    prediction_widget.value = dataset.categories[category_index]\n",
    "    for i, score in enumerate(list(output)):\n",
    "        score_widgets[i].value = score\n",
    "\n",
    "# during the live state the camera feed thread is run in real-time but and the 'Recognise' button is polled\n",
    "# inference is only performed by the recognise function when the button is pressed\n",
    "def live(state_widget, model, camera, prediction_widget, score_widget):\n",
    "    global dataset\n",
    "    while state_widget.value == 'Predict':\n",
    "        recognise_widget.on_click(recognise) \n",
    "                        \n",
    "def start_live(change):\n",
    "    if change['new'] == 'Predict':\n",
    "        execute_thread = threading.Thread(target=live, args=(state_widget, model, camera, prediction_widget, score_widget))\n",
    "        execute_thread.start()\n",
    "state_widget.observe(start_live, names='value')\n",
    "\n",
    "predict_widget = ipywidgets.VBox([ipywidgets.HBox(score_widgets),\n",
    "                                  ipywidgets.HBox([recognise_widget,prediction_widget])])\n",
    "\n",
    "# outputs\n",
    "print(\"state_widget and predict_widget created\")\n",
    "\n",
    "################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation\n",
    "Define the training widgets, after new data is collected the final layer of the model must be retrained to learn the new images. This cell may take several seconds to execute. The default number of epochs for training the model is 10. The first epoch takes a while to begin as the images are loaded into memmory, but then training proceeds relatively quickly, counting epochs down. Once training is complete the system automatically switches to the 'Predict' state and waits for the 'recognise' button to be pressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "\n",
    "epochs_widget = ipywidgets.IntText(description='epochs', value=10)\n",
    "eval_button = ipywidgets.Button(description='evaluate')\n",
    "train_button = ipywidgets.Button(description='train')\n",
    "loss_widget = ipywidgets.FloatText(description='loss')\n",
    "accuracy_widget = ipywidgets.FloatText(description='accuracy')\n",
    "progress_widget = ipywidgets.FloatProgress(min=0.0, max=1.0, description='progress')\n",
    "\n",
    "def train_eval(is_training):\n",
    "    global BATCH_SIZE, LEARNING_RATE, MOMENTUM, model, dataset, optimizer, eval_button, train_button, accuracy_widget, loss_widget, progress_widget, state_widget\n",
    "    \n",
    "    try:\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        state_widget.value = 'Train'\n",
    "        train_button.disabled = True\n",
    "        eval_button.disabled = True\n",
    "        time.sleep(1)\n",
    "\n",
    "        if is_training:\n",
    "            model = model.train()\n",
    "        else:\n",
    "            model = model.eval()\n",
    "        while epochs_widget.value > 0:\n",
    "            i = 0\n",
    "            sum_loss = 0.0\n",
    "            error_count = 0.0\n",
    "            for images, labels in iter(train_loader):\n",
    "                # send data to device\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                if is_training:\n",
    "                    # zero gradients of parameters\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # execute model to get outputs\n",
    "                outputs = model(images)\n",
    "\n",
    "                # compute loss\n",
    "                loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "                if is_training:\n",
    "                    # run backpropogation to accumulate gradients\n",
    "                    loss.backward()\n",
    "\n",
    "                    # step optimizer to adjust parameters\n",
    "                    optimizer.step()\n",
    "\n",
    "                # increment progress\n",
    "                error_count += len(torch.nonzero(outputs.argmax(1) - labels).flatten())\n",
    "                count = len(labels.flatten())\n",
    "                i += count\n",
    "                sum_loss += float(loss)\n",
    "                progress_widget.value = i / len(dataset)\n",
    "                loss_widget.value = sum_loss / i\n",
    "                accuracy_widget.value = 1.0 - error_count / i\n",
    "                \n",
    "            if is_training:\n",
    "                epochs_widget.value = epochs_widget.value - 1\n",
    "            else:\n",
    "                break\n",
    "    except e:\n",
    "        pass\n",
    "    model = model.eval()\n",
    "\n",
    "    train_button.disabled = False\n",
    "    eval_button.disabled = False\n",
    "    state_widget.value = 'Predict'\n",
    "    \n",
    "train_button.on_click(lambda c: train_eval(is_training=True))\n",
    "eval_button.on_click(lambda c: train_eval(is_training=False))\n",
    "\n",
    "training_widget = ipywidgets.VBox([ipywidgets.HBox([train_button,eval_button]),\n",
    "                                   epochs_widget,\n",
    "                                   progress_widget,\n",
    "                                   accuracy_widget]) \n",
    "\n",
    "# loss_widget not included\n",
    "\n",
    "# output\n",
    "print(\"training configured and training_widget created\")\n",
    "\n",
    "################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Project Controls\n",
    "Groups and arranges all the created widgets into the display for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define project controls #################################################################\n",
    "# constructed from:\n",
    "# camera_widget, data_collection_widget\n",
    "# modelsave_widget\n",
    "# state_widget, predict_widget and score_widgets\n",
    "# training_widget\n",
    "project_controls = ipywidgets.VBox([state_widget,\n",
    "                                    ipywidgets.HBox([camera_widget,predict_widget]),\n",
    "                                    ipywidgets.HBox([data_collection_widget,training_widget]),\n",
    "                                    model_widget])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the Project Controls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#FFC300;\"></h1>\n",
    "<center><img src=\"./BCCC_banner_814.png\" alt=\"Header\" style=\"width: 814px;\"/></center>\n",
    "<h1 style=\"background-color:#FFC300;\"></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(project_controls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#FFC300;\"></h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
